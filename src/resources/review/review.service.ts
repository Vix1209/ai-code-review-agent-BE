import { Injectable } from '@nestjs/common';
import { EmbeddingService } from '../embedding/embedding.service';
import { LlmService } from '../llm/llm.service';
import { VectorDbService } from '../vector-db/vector-db.service';
import { DatabaseService } from '../database/database.service';
import { GenerateReviewDto, SubmitReferenceDto } from './dto/resource.dto';

@Injectable()
export class ReviewService {
  constructor(
    private readonly embeddingService: EmbeddingService,
    private readonly vectorDbService: VectorDbService,
    private readonly llmService: LlmService,
    private readonly databaseService: DatabaseService,
  ) {}

  /**
   * Processes and stores a reference in the vector database
   * @bodyParam content - The text content to process
   * @bodyParam metadata - Metadata associated with the content
   * @returns Success response
   */
  async processReference(data: SubmitReferenceDto) {
    const { content, metadata, userId } = data;
    // Generate embedding for the content
    try {
      const embedding = await this.embeddingService.generateEmbedding(content);

      // Store embedding in the vector database
      await this.vectorDbService.upsertEmbedding({
        id: metadata.id || content.slice(0, 10),
        values: embedding,
        metadata: { ...metadata, content }, // Ensure content is stored
      });

      if (userId) {
        // Save reference to the database
        await this.databaseService.saveReference(
          content,
          embedding,
          metadata,
          userId,
        );
      }

      return { success: true, message: 'Reference stored successfully.' };
    } catch (error) {
      return {
        success: false,
        message: `Error processing reference: ${error.message}`,
      };
    }
  }

  /**
   * Generates a review using the LLM based on a prompt and context retrieved from the vector database
   * @body prompt - The user's query or prompt
   * @returns Feedback generated by the LLM
   */
  async generateReview(data: GenerateReviewDto) {
    const { prompt, userId } = data;

    // Generate embedding for the query prompt
    const queryEmbedding =
      await this.embeddingService.generateEmbedding(prompt);

    // Search for relevant embeddings in the vector database
    const searchResults = await this.vectorDbService.searchEmbedding(
      queryEmbedding,
      Number(process.env.TOP_K_VALUE) || 3,
    ); // Top matches to return

    // Combine context from search results
    const context = searchResults
      .map((result) => result.metadata?.content || '')
      .join('\n');

    // Enrich the prompt with retrieved context
    const enrichedPrompt = `${prompt}\n\nContext:\n${context}`;

    // Use the LLM to generate feedback
    const feedbackResponse =
      await this.llmService.generateFeedback(enrichedPrompt);

    if (feedbackResponse) {
      const feedback = feedbackResponse.content || ''; // Ensure feedback is always a string

      // Log the feedback for debugging

      if (userId) {
        // Update user table with review data
        await this.databaseService.saveReview(
          prompt,
          enrichedPrompt,
          feedback,
          userId,
        );
      }
      // Save review to the database

      return { feedback };
    } else {
      return { feedback: 'No feedback available.' };
    }
  }

  async getReviews() {
    const reviews = await this.databaseService.getAllReviews();
    return reviews;
  }

  async getSingleReview(id: string) {
    const reviews = await this.databaseService.getSingleReview(id);
    return reviews;
  }

  async getReferences() {
    const reference = await this.databaseService.getAllReferences();
    return reference;
  }

  async getSingleReference(id: string) {
    const reference = await this.databaseService.getSingleReference(id);
    return reference;
  }
}
