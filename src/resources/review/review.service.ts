import { Injectable } from '@nestjs/common';
import { EmbeddingService } from '../embedding/embedding.service';
import { LlmService } from '../llm/llm.service';
import { VectorDbService } from '../vector-db/vector-db.service';
import { DatabaseService } from '../database/database.service';

@Injectable()
export class ReviewService {
  constructor(
    private readonly embeddingService: EmbeddingService,
    private readonly vectorDbService: VectorDbService,
    private readonly llmService: LlmService,
    private readonly databaseService: DatabaseService,
  ) {}

  /**
   * Processes and stores a reference in the vector database
   * @bodyParam content - The text content to process
   * @bodyParam metadata - Metadata associated with the content
   * @returns Success response
   */
  async processReference(content: string, metadata: Record<string, any>) {
    // Generate embedding for the content
    try {
      const embedding = await this.embeddingService.generateEmbedding(content);
      console.log('Embedding: ', embedding);

      // Store embedding in the vector database
      await this.vectorDbService.upsertEmbedding({
        id: metadata.id || content.slice(0, 10), // Generate a unique ID if none is provided
        values: embedding,
        metadata,
      });

      // // Save reference to the database
      // await this.databaseService.saveReference(content, embedding, metadata);

      return { success: true, message: 'Reference stored successfully.' };
    } catch (error) {
      return {
        success: false,
        message: `Error processing reference: ${error.message}`,
      };
    }
  }

  /**
   * Generates a review using the LLM based on a prompt and context retrieved from the vector database
   * @body prompt - The user's query or prompt
   * @returns Feedback generated by the LLM
   */
  async generateReview(prompt: string) {
    // Generate embedding for the query prompt
    const queryEmbedding =
      await this.embeddingService.generateEmbedding(prompt);

    console.log('queryEmbedding: ', queryEmbedding);

    // Search for relevant embeddings in the vector database
    const searchResults = await this.vectorDbService.searchEmbedding(
      queryEmbedding,
      Number(process.env.TOP_K_VALUE) || 3,
    ); // Top matches to return

    console.log('searchResults, ', searchResults);
    // Combine context from search results
    const context = searchResults
      .map((result) => result.metadata?.content || '')
      .join('\n');

    // Enrich the prompt with retrieved context
    const enrichedPrompt = `${prompt}\n\nContext:\n${context}`;

    console.log('Context: ', context);

    // Use the LLM to generate feedback
    const feedbackResponse =
      await this.llmService.generateFeedback(enrichedPrompt);

    if (feedbackResponse) {
      const feedback = feedbackResponse.content || ''; // Ensure feedback is always a string

      // Log the feedback for debugging
      console.log('Feedback:', feedback);

      // Save review to the database
      // await this.databaseService.saveReview(prompt, enrichedPrompt, feedback);

      return { feedback };
    } else {
      return { feedback: 'No feedback available.' };
    }
  }

  async getReviews() {
    const reviews = await this.databaseService.getAllReviews();
    return reviews;
  }

  async getReferences() {
    const reference = await this.databaseService.getAllReferences();
    return reference;
  }
}
